{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from pmdarima.utils import decomposed_plot\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from pmdarima.arima import auto_arima\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Function to convert age range to a numerical value\n",
    "def convertir_rango_edad_a_numero(rango_edad):\n",
    "    return int(rango_edad // 3)\n",
    "\n",
    "# Function to calculate Mean Absolute Percentage Error (MAPE)\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Function to calculate various metrics from model results\n",
    "def calculo_metricas(results):\n",
    "    residuals = results.resid\n",
    "    \n",
    "    mae = np.mean(np.abs(residuals))\n",
    "    result = acorr_ljungbox(residuals, period=52)\n",
    "    mse = np.mean(residuals**2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    p_value = round(result.lb_pvalue.mean(), 3)\n",
    "    \n",
    "    return mae, mse, rmse, results.aic, results.bic, p_value\n",
    "\n",
    "# Function to create a summary table from model results\n",
    "def summary_table(results):\n",
    "    summary_table = results.summary().tables[1]\n",
    "    \n",
    "    # Convert the summary table to a DataFrame\n",
    "    summary_df = pd.DataFrame(summary_table.data[1:], columns=summary_table.data[0])\n",
    "    \n",
    "    # Clean up the DataFrame\n",
    "    summary_df = summary_df.apply(pd.to_numeric, errors='ignore')\n",
    "    \n",
    "    # Rename columns for better access\n",
    "    summary_df.columns = ['variable', 'coef', 'std_err', 'z', 'P>|z|', '[0.025', '0.975]']\n",
    "    \n",
    "    return summary_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f=pd.read_csv(\"Bases//Base_Limpia_Added.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Buenos Aires', 'CABA', 'Chaco', 'Cordoba', 'Formosa', 'Jujuy',\n",
       "       'La Pampa', 'Misiones', 'Salta', 'Santa Fe', 'Tucuman', 'San Luis',\n",
       "       'Catamarca', 'Entre Rios', 'San Juan', 'Tierra del Fuego',\n",
       "       'Corrientes', 'Neuquen', 'Chubut', 'Santiago del Estero',\n",
       "       'Mendoza', 'Santa Cruz', 'Rio Negro', 'La Rioja'], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_f[\"provincia\"].unique()\n",
    "\n",
    "df_c0=pd.read_csv(\"Bases//Base_Casos_Act.csv\")\n",
    "\n",
    "\n",
    "cols_clima=['salud',\"densidad_estimada\",\"index obesity\",\"TEMP\",\"HUM\",\"PNM\",\"DD\",\"FF\"]\n",
    "\n",
    "\n",
    "# After a lot of runs the poverty data didnt improve the model, this could is probably because is not temporal data, it only refer to one moment of the census\n",
    "cols_pobreza=['% de hogares sin acceso a red cloacal','% de población en situación de pobreza crónica','% de población sin obra social ni prepaga'] # Mejor\n",
    "cols_pobreza=['% de hogares sin acceso a red cloacal','% de población en situación de pobreza crónica']\n",
    "cols_pobreza=[ 'Nivel de incidencia de pobreza crónica','% de hogares con hacinamiento crítico','% de hogares en vivienda deficitaria','% de hogares sin acceso a red cloacal','% de población en situación de pobreza crónica','% de población sin obra social ni prepaga']\n",
    "\n",
    "cols_pobreza=['% de hogares sin acceso a red cloacal','% de población en situación de pobreza crónica','% de población sin obra social ni prepaga'] # Mejor\n",
    "cols_pobreza=[ 'Nivel de incidencia de pobreza crónica','% de hogares con hacinamiento crítico','% de hogares en vivienda deficitaria','% de hogares sin acceso a red cloacal','% de población en situación de pobreza crónica','% de población sin obra social ni prepaga']\n",
    "\n",
    "cols_pobreza=[]\n",
    "cols_pred=cols_clima+cols_pobreza\n",
    "\n",
    "# Select the province or provinces for run the models\n",
    "l_provincias=[]\n",
    "\n",
    "l_provincias=[\"Santa Fe\"]\n",
    "\n",
    "l_provincias=[\"CABA\"]\n",
    "\n",
    "l_provincias=[\"Salta\",\"Jujuy\"]\n",
    "\n",
    "l_provincias=[\"Buenos Aires\",\"CABA\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA and SARIMAX Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(l_p, df_c0, df_f, cols_pred):\n",
    "    \"\"\"\n",
    "    Pre-processes tuberculosis incidence and climate data for specific provinces.\n",
    "\n",
    "    Input:\n",
    "    - l_p (list): List of provinces to filter data. If empty, processes data for all provinces.\n",
    "    - df_c0 (DataFrame): DataFrame containing COVID-19 data.\n",
    "    - df_f (DataFrame): DataFrame containing climate and poverty data.\n",
    "    - cols_pred (list): List of columns to use as predictors in the analysis.\n",
    "\n",
    "    Output:\n",
    "    - df_f2 (DataFrame): Processed DataFrame containing normalized climate and poverty data along with COVID-19 cases.\n",
    "    - promedio_por_dia (DataFrame): DataFrame with daily averages of predictors.\n",
    "    - promedio_por_semana (DataFrame): DataFrame with weekly averages of predictors.\n",
    "\n",
    "    Steps:\n",
    "    1. Filters data based on the provinces provided in `l_p` or uses all data if `l_p` is empty.\n",
    "    2. Normalizes columns in `df_f` except 'fecha' using MinMaxScaler.\n",
    "    3. Computes daily and weekly averages of predictors from `df_f`.\n",
    "    4. Interpolates missing values in `df_f2`.\n",
    "    \"\"\"\n",
    "    # Step 1: Filter data based on provinces\n",
    "    if len(l_p) == 0:\n",
    "        df_cc = df_c0.copy()\n",
    "        df_f2 = df_f.copy()[cols_pred + [\"fecha\"]]\n",
    "    else:\n",
    "        df_cc = df_c0.loc[df_c0[\"provincia\"].isin(l_p)]\n",
    "        df_f2 = df_f.loc[df_f[\"provincia\"].isin(l_p)][cols_pred + [\"fecha\"]]\n",
    "\n",
    "    # Drop 'provincia' column\n",
    "    df_cc.drop([\"provincia\"], axis=1, inplace=True)\n",
    "\n",
    "    # Step 2: Normalize columns in df_f2 except 'fecha'\n",
    "    columns_to_normalize = df_f2.columns.difference(['fecha'])\n",
    "    scaler = MinMaxScaler()\n",
    "    df_f2 = df_f2.copy()\n",
    "    df_f2[columns_to_normalize] = scaler.fit_transform(df_f2[columns_to_normalize])\n",
    "    df_f2['fecha'] = pd.to_datetime(df_f2['fecha'])\n",
    "\n",
    "    # Step 3: Compute daily and weekly averages of predictors in df_f2\n",
    "    promedio_por_dia = df_f2.groupby(df_f2['fecha'].dt.to_period('D'))[cols_pred].mean()\n",
    "    promedio_por_semana = df_f2.groupby(df_f2['fecha'].dt.to_period('W-Mon'))[cols_pred].mean()\n",
    "\n",
    "    # Step 4: Merge COVID-19 cases with climate and poverty data by week\n",
    "    df_cc['fecha'] = pd.to_datetime(df_cc['fecha'])\n",
    "    incidencia_por_semana2 = df_cc.groupby(df_cc['fecha'].dt.to_period('W-Mon'))['casos_corr_2'].sum()\n",
    "    incidencia_por_semana2 = incidencia_por_semana2.reset_index()\n",
    "\n",
    "    df_f2 = pd.merge(pd.DataFrame(incidencia_por_semana2, columns=[\"fecha\", \"casos_corr_2\"]),\n",
    "                     pd.DataFrame(promedio_por_semana), on=\"fecha\", how=\"left\")\n",
    "    df_f2 = df_f2.rename(columns={\"casos_corr_2\": \"Casos\"})\n",
    "\n",
    "    # Additional processing: Convert 'fecha' to datetime and interpolate missing values\n",
    "    df_f2['fecha'] = pd.to_datetime(df_f2['fecha'].astype(str).str.split('/').str[0])\n",
    "    df_f2 = df_f2.interpolate()\n",
    "\n",
    "    return df_f2, promedio_por_dia, promedio_por_semana\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Buenos Aires', 'CABA']\n",
      "ARIMA results\n",
      "(3.2811015473091207, 17.86447668909888, 4.226638935265098, 4173.663039661218, 4187.442173263646, 0.003)\n",
      "ARIMAX results\n",
      "                               SARIMAX Results                                \n",
      "==============================================================================\n",
      "Dep. Variable:                  Casos   No. Observations:                  731\n",
      "Model:                 ARIMA(1, 1, 1)   Log Likelihood               -2028.248\n",
      "Date:                Wed, 26 Jun 2024   AIC                           4078.497\n",
      "Time:                        19:11:40   BIC                           4129.020\n",
      "Sample:                             0   HQIC                          4097.989\n",
      "                                - 731                                         \n",
      "Covariance Type:                  opg                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "salud                 5.8034      6.853      0.847      0.397      -7.629      19.235\n",
      "densidad_estimada    19.1755      4.213      4.552      0.000      10.918      27.433\n",
      "index obesity         4.5934      7.965      0.577      0.564     -11.017      20.204\n",
      "TEMP                 -1.0047      2.113     -0.475      0.634      -5.146       3.137\n",
      "HUM                  -1.4436      2.544     -0.567      0.570      -6.431       3.543\n",
      "PNM                  -2.5540      2.412     -1.059      0.290      -7.282       2.174\n",
      "DD                   -2.4839      2.360     -1.052      0.293      -7.110       2.142\n",
      "FF                    2.7818      3.602      0.772      0.440      -4.277       9.841\n",
      "ar.L1                 0.4361      0.040     10.855      0.000       0.357       0.515\n",
      "ma.L1                -0.9111      0.020    -46.299      0.000      -0.950      -0.872\n",
      "sigma2               15.1520      0.671     22.574      0.000      13.836      16.468\n",
      "===================================================================================\n",
      "Ljung-Box (L1) (Q):                   3.73   Jarque-Bera (JB):                75.24\n",
      "Prob(Q):                              0.05   Prob(JB):                         0.00\n",
      "Heteroskedasticity (H):               0.98   Skew:                             0.46\n",
      "Prob(H) (two-sided):                  0.88   Kurtosis:                         4.28\n",
      "===================================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n",
      "(2.996130493635936, 15.265165625242753, 3.907066114777526, 4078.4968766829556, 4129.020366558522, 0.001)\n",
      "Auto model results\n",
      "                               SARIMAX Results                                \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  731\n",
      "Model:               SARIMAX(2, 1, 1)   Log Likelihood               -2076.503\n",
      "Date:                Wed, 26 Jun 2024   AIC                           4161.006\n",
      "Time:                        19:11:46   BIC                           4179.378\n",
      "Sample:                             0   HQIC                          4168.094\n",
      "                                - 731                                         \n",
      "Covariance Type:                  opg                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "ar.L1          0.4371      0.036     12.268      0.000       0.367       0.507\n",
      "ar.L2          0.1557      0.035      4.399      0.000       0.086       0.225\n",
      "ma.L1         -0.9564      0.014    -67.895      0.000      -0.984      -0.929\n",
      "sigma2        17.2803      0.753     22.949      0.000      15.804      18.756\n",
      "===================================================================================\n",
      "Ljung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):                53.31\n",
      "Prob(Q):                              0.98   Prob(JB):                         0.00\n",
      "Heteroskedasticity (H):               1.07   Skew:                             0.45\n",
      "Prob(H) (two-sided):                  0.59   Kurtosis:                         3.97\n",
      "===================================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n",
      "SARIMA results\n",
      "(3.250108700414372, 17.510684391995344, 4.184576966910197, 4161.006245183575, 4179.378423320144, 0.03)\n",
      "SARIMAX results\n",
      "                               SARIMAX Results                                \n",
      "==============================================================================\n",
      "Dep. Variable:                  Casos   No. Observations:                  731\n",
      "Model:               SARIMAX(2, 1, 1)   Log Likelihood               -2014.503\n",
      "Date:                Wed, 26 Jun 2024   AIC                           4053.006\n",
      "Time:                        19:11:48   BIC                           4108.122\n",
      "Sample:                             0   HQIC                          4074.270\n",
      "                                - 731                                         \n",
      "Covariance Type:                  opg                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "salud                 5.9981      6.582      0.911      0.362      -6.903      18.899\n",
      "densidad_estimada    22.4695      4.140      5.428      0.000      14.356      30.583\n",
      "index obesity         5.6857      7.523      0.756      0.450      -9.060      20.431\n",
      "TEMP                 -0.9032      2.262     -0.399      0.690      -5.336       3.530\n",
      "HUM                  -1.3798      2.619     -0.527      0.598      -6.512       3.753\n",
      "PNM                  -1.7937      2.435     -0.737      0.461      -6.567       2.979\n",
      "DD                   -3.2519      2.495     -1.304      0.192      -8.141       1.638\n",
      "FF                    2.3703      3.627      0.654      0.513      -4.738       9.478\n",
      "ar.L1                 0.3967      0.035     11.257      0.000       0.328       0.466\n",
      "ar.L2                 0.2334      0.036      6.481      0.000       0.163       0.304\n",
      "ma.L1                -0.9664      0.013    -72.143      0.000      -0.993      -0.940\n",
      "sigma2               14.5121      0.657     22.095      0.000      13.225      15.799\n",
      "===================================================================================\n",
      "Ljung-Box (L1) (Q):                   0.02   Jarque-Bera (JB):                76.46\n",
      "Prob(Q):                              0.88   Prob(JB):                         0.00\n",
      "Heteroskedasticity (H):               0.98   Skew:                             0.47\n",
      "Prob(H) (two-sided):                  0.89   Kurtosis:                         4.27\n",
      "===================================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n",
      "(2.9428612520228947, 14.686746117675117, 3.832329072205976, 4053.0058745222605, 4108.122408931969, 0.032)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "      <th>AIC</th>\n",
       "      <th>BIC</th>\n",
       "      <th>p-value</th>\n",
       "      <th>Model Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.281102</td>\n",
       "      <td>17.864477</td>\n",
       "      <td>4.226639</td>\n",
       "      <td>4173.663040</td>\n",
       "      <td>4187.442173</td>\n",
       "      <td>0.003</td>\n",
       "      <td>ARIMA (1,1,1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.996130</td>\n",
       "      <td>15.265166</td>\n",
       "      <td>3.907066</td>\n",
       "      <td>4078.496877</td>\n",
       "      <td>4129.020367</td>\n",
       "      <td>0.001</td>\n",
       "      <td>ARIMA(1,1,1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.250109</td>\n",
       "      <td>17.510684</td>\n",
       "      <td>4.184577</td>\n",
       "      <td>4161.006245</td>\n",
       "      <td>4179.378423</td>\n",
       "      <td>0.030</td>\n",
       "      <td>SARIMAX (2, 1, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.942861</td>\n",
       "      <td>14.686746</td>\n",
       "      <td>3.832329</td>\n",
       "      <td>4053.005875</td>\n",
       "      <td>4108.122409</td>\n",
       "      <td>0.032</td>\n",
       "      <td>SARIMAX (2, 1, 1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mae        mse      rmse          AIC          BIC  p-value  \\\n",
       "0  3.281102  17.864477  4.226639  4173.663040  4187.442173    0.003   \n",
       "1  2.996130  15.265166  3.907066  4078.496877  4129.020367    0.001   \n",
       "2  3.250109  17.510684  4.184577  4161.006245  4179.378423    0.030   \n",
       "3  2.942861  14.686746  3.832329  4053.005875  4108.122409    0.032   \n",
       "\n",
       "          Model Name  \n",
       "0      ARIMA (1,1,1)  \n",
       "1       ARIMA(1,1,1)  \n",
       "2  SARIMAX (2, 1, 1)  \n",
       "3  SARIMAX (2, 1, 1)  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def easy_models(df_f2, cols_pred):\n",
    "    \"\"\"\n",
    "    Runs and evaluates multiple time series models on COVID-19 data.\n",
    "\n",
    "    Input:\n",
    "    - df_f2 (DataFrame): Processed DataFrame containing normalized climate, poverty, and COVID-19 case data.\n",
    "    - cols_pred (list): List of columns used as predictors in the analysis.\n",
    "\n",
    "    Output:\n",
    "    - arima_results (str): Summary table of ARIMA model results.\n",
    "    - arimax_results (str): Summary table of ARIMAX model results.\n",
    "    - sarima_results (str): Summary table of SARIMA model results.\n",
    "    - sarimax_results (str): Summary table of SARIMAX model results.\n",
    "    - df_metrics (DataFrame): DataFrame containing evaluation metrics (MAE, MSE, RMSE, AIC, BIC, p-value) for each model.\n",
    "    \"\"\"\n",
    "    # ARIMA model without exogenous variables\n",
    "    model = sm.tsa.ARIMA(df_f2['Casos'], order=(1, 1, 1))\n",
    "    results = model.fit()\n",
    "    print(\"ARIMA results\")\n",
    "    arima_results = summary_table(results)\n",
    "    arima_metrics = calculo_metricas(results)\n",
    "    print(arima_metrics)\n",
    "\n",
    "    # Adjust first fitted value to mean\n",
    "    results.fittedvalues[0] = results.fittedvalues.mean()\n",
    "\n",
    "    # ARIMAX model with exogenous variables\n",
    "    model = sm.tsa.ARIMA(df_f2['Casos'], exog=df_f2[cols_pred], order=(1, 1, 1))\n",
    "    results = model.fit()\n",
    "    print(\"ARIMAX results\")\n",
    "    arimax_results = summary_table(results)\n",
    "    print(results.summary())\n",
    "    arimax_metrics = calculo_metricas(results)\n",
    "    print(arimax_metrics)\n",
    "\n",
    "    # Auto ARIMA model selection\n",
    "    print(\"Auto model results\")\n",
    "    auto_model = auto_arima(df_f2['Casos'], max_p=8, max_d=8, max_q=8)\n",
    "    print(auto_model.summary())\n",
    "    p_best = auto_model.order[0]\n",
    "    d_best = auto_model.order[1]\n",
    "    q_best = auto_model.order[2]\n",
    "\n",
    "    # SARIMA model\n",
    "    order = (p_best, d_best, q_best)\n",
    "    model = sm.tsa.SARIMAX(df_f2['Casos'], order=order)\n",
    "    results = model.fit()\n",
    "    sarima_results = summary_table(results)\n",
    "    print(\"SARIMA results\")\n",
    "    results.fittedvalues[0] = results.fittedvalues.mean()\n",
    "    sarima_metrics = calculo_metricas(results)\n",
    "    print(sarima_metrics)\n",
    "\n",
    "    # SARIMAX model with exogenous variables\n",
    "    model = sm.tsa.SARIMAX(df_f2['Casos'], order=order, exog=df_f2[cols_pred])\n",
    "    results = model.fit()\n",
    "    sarimax_results = summary_table(results)\n",
    "    print(\"SARIMAX results\")\n",
    "    print(results.summary())\n",
    "    results.fittedvalues[0] = results.fittedvalues.mean()\n",
    "    sarimax_metrics = calculo_metricas(results)\n",
    "    print(sarimax_metrics)\n",
    "\n",
    "    # Combine metrics into DataFrame for easy comparison\n",
    "    l_metrics = [arima_metrics, arimax_metrics, sarima_metrics, sarimax_metrics]\n",
    "    names = [\"ARIMA (1,1,1)\", \"ARIMAX (1,1,1)\", f\"SARIMA ({p_best},{d_best},{q_best})\", f\"SARIMAX ({p_best},{d_best},{q_best})\"]\n",
    "    df_metrics = pd.DataFrame(l_metrics, columns=[\"mae\", \"mse\", \"rmse\", \"AIC\", \"BIC\", \"p-value\"])\n",
    "    df_metrics[\"Model Name\"] = names\n",
    "\n",
    "    return arima_results, arimax_results, sarima_results, sarimax_results, df_metrics\n",
    "\n",
    "\n",
    "def grid_search(df_f2, best_rmse, model_=\"SARIMAX\"):\n",
    "    \"\"\"\n",
    "    Performs a grid search to find the best SARIMA/SARIMAX model based on RMSE and p-value criteria.\n",
    "\n",
    "    Input:\n",
    "    - df_f2 (DataFrame): Processed DataFrame containing normalized climate, poverty, and COVID-19 case data.\n",
    "    - best_rmse (float): Best RMSE value obtained from initial model evaluation.\n",
    "    - model_ (str): Type of model to perform grid search ('SARIMAX' or 'ARIMA').\n",
    "\n",
    "    Output:\n",
    "    - df_best (DataFrame): DataFrame containing metrics and results of best-performing models from grid search.\n",
    "    \"\"\"\n",
    "    l_metrics = []\n",
    "    p, d, q = 4, 2, 4  # Define search ranges for p, d, q\n",
    "    print(f\"max p, d, q: {p}, {d}, {q}\")\n",
    "\n",
    "    # Perform grid search over defined parameter ranges\n",
    "    for p_ in range(p):\n",
    "        for d_ in range(d):\n",
    "            for q_ in range(q):\n",
    "                order = (p_, d_, q_)  # ARIMA/SARIMA order parameters\n",
    "\n",
    "                try:\n",
    "                    if model_ == \"SARIMAX\":\n",
    "                        model = sm.tsa.SARIMAX(df_f2['Casos'], exog=df_f2[cols_pred], order=order)\n",
    "                    elif model_ == \"ARIMA\":\n",
    "                        model = sm.tsa.ARIMA(df_f2['Casos'], exog=df_f2[cols_pred], order=order)\n",
    "\n",
    "                    results = model.fit()\n",
    "                    results.fittedvalues[0] = results.fittedvalues.mean()  # Adjust first fitted value\n",
    "\n",
    "                    # Calculate metrics\n",
    "                    mae, mse, rmse, aic, bic, p_value = calculo_metricas(results)\n",
    "\n",
    "                    # Store results if RMSE is lower than current best and p-value is significant\n",
    "                    if rmse <= best_rmse and p_value < 0.1:\n",
    "                        best_results = summary_table(results)\n",
    "                        print(p_, d_, q_)\n",
    "                        if p_value < 0.06:\n",
    "                            print(results.summary())\n",
    "                        print(mae, mse, rmse, aic, bic, p_value)\n",
    "                        values = (p_, d_, q_, mae, mse, rmse, aic, bic, p_value, best_results)\n",
    "                        l_metrics.append(values)\n",
    "\n",
    "                except:\n",
    "                    print(\"Error encountered in model fitting\")\n",
    "\n",
    "    # Create DataFrame from collected metrics and results\n",
    "    df_best = pd.DataFrame(l_metrics)\n",
    "    df_best[\"NAME\"] = f\"SARIMAX ({df_best[0].astype(str)}, {df_best[1].astype(str)}, {df_best[2].astype(str)})\"\n",
    "    df_best.drop([0, 1, 2], axis=1, inplace=True)\n",
    "    df_best.columns = [\"mae\", \"mse\", \"rmse\", \"AIC\", \"BIC\", \"p-value\", \"results\", \"Model Name\"]\n",
    "\n",
    "    return df_best\n",
    "\n",
    "\n",
    "def best_sarimax(df_metrics, model_):\n",
    "    \"\"\"\n",
    "    Identifies the best SARIMA/SARIMAX model based on p-value and RMSE criteria.\n",
    "\n",
    "    Input:\n",
    "    - df_metrics (DataFrame): DataFrame containing evaluation metrics for multiple SARIMA/SARIMAX models.\n",
    "    - model_ (str): Type of model to consider ('SARIMAX' or 'ARIMA').\n",
    "\n",
    "    Output:\n",
    "    - df_metrics (DataFrame): Updated DataFrame with details of the best-performing SARIMA/SARIMAX model.\n",
    "    - df_best (DataFrame): DataFrame containing metrics and results of best-performing models.\n",
    "    \"\"\"\n",
    "    # Display metrics for models with p-value < 0.05\n",
    "    display(df_metrics[df_metrics[\"p-value\"] < 0.05])\n",
    "\n",
    "    # Find the best RMSE among models with significant p-value\n",
    "    best_rmse = df_metrics.loc[df_metrics[\"p-value\"] < 0.05][\"rmse\"].min()\n",
    "\n",
    "    # Perform grid search to find best model based on updated RMSE criteria\n",
    "    df_best = grid_search(df_f2, best_rmse, model_)\n",
    "\n",
    "    # Identify the best SARIMAX model with p-value < 0.06 and lowest RMSE\n",
    "    best_sarimax = df_best.loc[df_best[\"p-value\"] < 0.06].sort_values(by=\"rmse\").iloc[0]\n",
    "\n",
    "    # Append best SARIMAX model details to df_metrics and round values for clarity\n",
    "    df_metrics.loc[-1] = best_sarimax\n",
    "    df_metrics = df_metrics.reset_index(drop=True).round(2)\n",
    "\n",
    "    return df_metrics, df_best\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "for l_p in [l_provincias]:\n",
    "    print(l_p)\n",
    "    df_f2, promedio_dia, promedio_por_semana = pre_process(l_p, df_c0, df_f, cols_pred)\n",
    "    arima_results, arimax_results, sarima_results, sarimax_results, df_metrics = easy_models(df_f2, cols_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.832329072205976"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rmse=df_metrics.loc[df_metrics[\"p-value\"]<0.05][\"rmse\"].min()\n",
    "best_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_table_to_df(simple_table):\n",
    "    \"\"\"\n",
    "    Convert a simple table structure into a Pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - simple_table (object): A structured data object representing a table.\n",
    "    \n",
    "    Returns:\n",
    "    - df (DataFrame): Pandas DataFrame containing the table data.\n",
    "    \"\"\"\n",
    "    data = simple_table.data[1:]  # Skip the header row, assuming data starts from the second row\n",
    "    headers = simple_table.data[0]  # Use the first row as headers\n",
    "    \n",
    "    # Create DataFrame with extracted data and headers\n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "    \n",
    "    # Convert all columns to numeric (ignore errors if conversion fails)\n",
    "    df = df.apply(pd.to_numeric, errors='ignore')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def best_variables(arimax_results, sarimax_results, best_sarimax):\n",
    "    \"\"\"\n",
    "    Consolidates and compares coefficients and statistical significance (P-values) \n",
    "    of variables from different model results (ARIMAX, SARIMAX, and best-performing SARIMAX).\n",
    "\n",
    "    Inputs:\n",
    "    - arimax_results: Summary table or data structure from ARIMAX model results.\n",
    "    - sarimax_results: Summary table or data structure from SARIMAX model results.\n",
    "    - best_sarimax: Detailed results from the best-performing SARIMAX model.\n",
    "\n",
    "    Outputs:\n",
    "    - df_final: Pandas DataFrame with variables as rows and columns representing \n",
    "      coefficients (coef) and statistical significance (P>|z|) from ARIMAX, SARIMAX, \n",
    "      and best-performing SARIMAX models.\n",
    "    \"\"\"\n",
    "    # Extract relevant columns from ARIMAX and SARIMAX results\n",
    "    df1 = simple_table_to_df(arimax_results)[[\"\", \"coef\", \"P>|z|\"]].add_suffix('_ARIMAX')\n",
    "    df2 = simple_table_to_df(sarimax_results)[[\"\", \"coef\", \"P>|z|\"]].add_suffix('_SARIMAX')\n",
    "    \n",
    "    # Extract relevant columns from best SARIMAX results\n",
    "    df3 = simple_table_to_df(best_sarimax[\"results\"])[[\"\", \"coef\", \"P>|z|\"]].add_suffix('_SARIMAX_BEST')\n",
    "\n",
    "    # Rename columns for merging and comparison\n",
    "    df1 = df1.rename(columns={'_ARIMAX': 'variable'})\n",
    "    df2 = df2.rename(columns={'_SARIMAX': 'variable'})\n",
    "    df3 = df3.rename(columns={'_SARIMAX_BEST': 'variable'})\n",
    "\n",
    "    # Merge DataFrames on 'variable' column\n",
    "    df_final = pd.merge(df1, df2, on=\"variable\", how=\"left\")\n",
    "    df_final = pd.merge(df_final, df3, on=\"variable\", how=\"left\")[:-3]  # Remove last 3 rows (assuming they are footer rows)\n",
    "\n",
    "    # Filter final DataFrame based on significance levels\n",
    "    df_final = df_final.loc[(df_final[\"P>|z|_ARIMAX\"] < 0.15) | (df_final[\"P>|z|_SARIMAX\"] < 0.05) | (df_final[\"P>|z|_SARIMAX_BEST\"] < 0.15)]\n",
    "\n",
    "    return df_final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter df_metrics to select rows where the model name starts with \"SARIMAX\"\n",
    "# and the p-value is less than or equal to 0.1, indicating statistical significance.\n",
    "sarimax_true = df_metrics.loc[(df_metrics[\"Model Name\"].str[:7] == \"SARIMAX\") & (df_metrics[\"p-value\"] <= 0.1)]\n",
    "\n",
    "# Specify the model type to focus on for further analysis and comparison.\n",
    "model_best = \"SARIMAX\"\n",
    "\n",
    "# Call the function best_sarimax to identify and extract the best-performing SARIMAX model\n",
    "# from df_metrics based on predefined criteria.\n",
    "df_metrics, df_best = best_sarimax(df_metrics, model_best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df_metrics DataFrame to a CSV file with a filename based on l_provincias\n",
    "df_metrics.to_csv(\"Resultados\\\\Tseries_Metricas\" + str(l_provincias) + \".csv\", index=False)\n",
    "\n",
    "# Extract the detailed results DataFrame of the best-performing model (last row in df_best) and format it\n",
    "df_variables = pd.DataFrame(df_best.iloc[-1][\"results\"])\n",
    "\n",
    "# Rename the first column to \"variable\" and set it as the header row\n",
    "df_variables[0].iloc[0] = \"variable\"\n",
    "df_variables.columns = df_variables.iloc[0]\n",
    "\n",
    "# Remove the first row (header row) to keep only the data rows\n",
    "df_variables = df_variables[1:]\n",
    "\n",
    "# Save df_variables DataFrame to a CSV file with a filename based on l_provincias\n",
    "df_variables.to_csv(\"Resultados\\\\Tseries_Variables\" + str(l_provincias) + \".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Buenos Aires', 'CABA']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "      <th>AIC</th>\n",
       "      <th>BIC</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARIMA (1,1,1)</td>\n",
       "      <td>3.28</td>\n",
       "      <td>17.86</td>\n",
       "      <td>4.23</td>\n",
       "      <td>4173</td>\n",
       "      <td>4187</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ARIMA(1,1,1)</td>\n",
       "      <td>3.00</td>\n",
       "      <td>15.27</td>\n",
       "      <td>3.91</td>\n",
       "      <td>4078</td>\n",
       "      <td>4129</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SARIMAX (2, 1, 1)</td>\n",
       "      <td>3.25</td>\n",
       "      <td>17.51</td>\n",
       "      <td>4.18</td>\n",
       "      <td>4161</td>\n",
       "      <td>4179</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SARIMAX (2, 1, 1)</td>\n",
       "      <td>2.94</td>\n",
       "      <td>14.69</td>\n",
       "      <td>3.83</td>\n",
       "      <td>4053</td>\n",
       "      <td>4108</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SARIMAX (3, 1, 2)</td>\n",
       "      <td>2.93</td>\n",
       "      <td>14.60</td>\n",
       "      <td>3.82</td>\n",
       "      <td>4053</td>\n",
       "      <td>4117</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model Name   mae    mse  rmse   AIC   BIC  p-value\n",
       "0      ARIMA (1,1,1)  3.28  17.86  4.23  4173  4187     0.00\n",
       "1       ARIMA(1,1,1)  3.00  15.27  3.91  4078  4129     0.00\n",
       "2  SARIMAX (2, 1, 1)  3.25  17.51  4.18  4161  4179     0.03\n",
       "3  SARIMAX (2, 1, 1)  2.94  14.69  3.83  4053  4108     0.03\n",
       "4  SARIMAX (3, 1, 2)  2.93  14.60  3.82  4053  4117     0.03"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the variables DataFrame from the CSV file and round numeric values to two decimal places\n",
    "df_variables = pd.read_csv(\"Resultados\\\\Tseries_Variables\" + str(l_provincias) + \".csv\").round(2)\n",
    "\n",
    "# Check if there is an 'Unnamed: 0' column in df_variables and drop it if present\n",
    "if \"Unnamed: 0\" in df_variables.columns:\n",
    "    df_variables.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "\n",
    "# Read the metrics DataFrame from the CSV file and round numeric values to two decimal places\n",
    "df_metrics = pd.read_csv(\"Resultados\\\\Tseries_Metricas\" + str(l_provincias) + \".csv\").round(2)\n",
    "\n",
    "# Check if there is an 'Unnamed: 0' column in df_metrics and drop it if present\n",
    "if \"Unnamed: 0\" in df_metrics.columns:\n",
    "    df_metrics.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "\n",
    "# Convert columns 'AIC' and 'BIC' in df_metrics to integer type\n",
    "for s in [\"AIC\", \"BIC\"]:\n",
    "    df_metrics[s] = df_metrics[s].astype(int)\n",
    "\n",
    "# Create a copy of df_metrics for auxiliary purposes\n",
    "df_aux = df_metrics.copy()\n",
    "\n",
    "# Drop the 'Model Name' column from df_metrics\n",
    "df_metrics.drop(\"Model Name\", axis=1, inplace=True)\n",
    "\n",
    "# Concatenate the 'Model Name' column back to df_metrics after processing\n",
    "df_metrics = pd.concat([df_aux[[\"Model Name\"]], df_metrics], axis=1)\n",
    "\n",
    "# Print the list of provinces l_provincias\n",
    "print(l_provincias)\n",
    "\n",
    "# Display or return df_metrics for further analysis or visualization\n",
    "df_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - loss: 0.0182 - val_loss: 0.0026\n",
      "Epoch 2/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0023 - val_loss: 8.2783e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0017 - val_loss: 7.4448e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0015 - val_loss: 7.0001e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0015 - val_loss: 7.1834e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 7/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0015 - val_loss: 7.4272e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0016 - val_loss: 9.6298e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0016 - val_loss: 0.0012\n",
      "Epoch 10/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0015 - val_loss: 9.1161e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0015 - val_loss: 7.7641e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 13/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 14/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0014 - val_loss: 7.6594e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0017 - val_loss: 0.0010\n",
      "Epoch 16/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0017 - val_loss: 8.4891e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 18/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0014 - val_loss: 8.6570e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 20/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 21/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0015 - val_loss: 7.3673e-04\n",
      "Epoch 22/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0016 - val_loss: 9.4663e-04\n",
      "Epoch 23/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 24/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017 - val_loss: 7.2133e-04\n",
      "Epoch 25/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0016 - val_loss: 7.3935e-04\n",
      "Epoch 26/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0016 - val_loss: 9.2398e-04\n",
      "Epoch 27/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0013 - val_loss: 7.4541e-04\n",
      "Epoch 28/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 29/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0015 - val_loss: 8.1883e-04\n",
      "Epoch 30/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0014 - val_loss: 7.7580e-04\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">43,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">80,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">80,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">80,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">101</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_8 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │        \u001b[38;5;34m43,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_9 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │        \u001b[38;5;34m80,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_10 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │        \u001b[38;5;34m80,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_11 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m80,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m101\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">853,505</span> (3.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m853,505\u001b[0m (3.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">284,501</span> (1.09 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m284,501\u001b[0m (1.09 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">569,004</span> (2.17 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m569,004\u001b[0m (2.17 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000223894D63E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 502ms/stepWARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000223894D63E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 133ms/step\n",
      "Mean Squared Error: 0.0007758029994051187\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Filter df_c0 by provinces in l_p\n",
    "df_cc = df_c0.loc[df_c0[\"provincia\"].isin(l_p)]\n",
    "\n",
    "# Convert 'fecha' column in df_f2 to datetime\n",
    "df_f2['fecha'] = pd.to_datetime(df_f2['fecha'])\n",
    "\n",
    "# Aggregate daily cases into weekly using W-Mon frequency\n",
    "df_daily = df_f2.groupby(df_f2['fecha'].dt.to_period('W-Mon'))['Casos'].sum().reset_index(name='cases')\n",
    "\n",
    "# Scale 'cases' using MinMaxScaler to range [0, 1]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df_daily['cases_scaled'] = scaler.fit_transform(df_daily['cases'].values.reshape(-1, 1))\n",
    "\n",
    "# Convert promedio_dia and promedio_por_semana to DataFrames\n",
    "prom_dia_variables = pd.DataFrame(promedio_dia.to_records())\n",
    "promedio_por_semana = pd.DataFrame(promedio_por_semana.to_records())\n",
    "\n",
    "# Convert 'fecha' column to string for merging\n",
    "prom_dia_variables[\"fecha\"] = prom_dia_variables[\"fecha\"].astype(str)\n",
    "df_ccc = df_cc[[\"fecha\", \"casos_corr_2\"]]\n",
    "df_ccc[\"fecha\"] = df_ccc[\"fecha\"].astype(str)\n",
    "\n",
    "# Merge prom_dia_variables and df_ccc on 'fecha'\n",
    "df_data = pd.merge(prom_dia_variables, df_ccc, on=[\"fecha\"], how=\"left\")\n",
    "\n",
    "# Set 'fecha' column as index and drop it from DataFrame\n",
    "df_data.index = df_data[\"fecha\"]\n",
    "df_data = pd.DataFrame(df_data.drop(\"fecha\", axis=1))\n",
    "\n",
    "# Function to create sequences for LSTM input\n",
    "def create_sequences(df, seq_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(len(df) - seq_length):\n",
    "        x = df.iloc[i:(i + seq_length), 1:].values  # Predictor variables (x1, x2, ..., xn)\n",
    "        y = df.iloc[i + seq_length, 0]              # Target time series (y)\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "# Sequence length for LSTM model\n",
    "seq_length = 10\n",
    "\n",
    "# Impute missing values in predictor variables\n",
    "imputer_X = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer_X.fit_transform(df_data.drop(columns=[\"casos_corr_2\"]).values)\n",
    "\n",
    "# Impute NaN values in target variable\n",
    "imputer_y = SimpleImputer(strategy='mean')\n",
    "y_imputed = imputer_y.fit_transform(df_data[\"casos_corr_2\"].values.reshape(-1, 1))\n",
    "\n",
    "# Create sequences for LSTM input\n",
    "X_seq, y_seq = create_sequences(pd.DataFrame(X_imputed), seq_length)\n",
    "\n",
    "# Split data into training and test sets\n",
    "split_ratio = 0.8\n",
    "split = int(len(X_seq) * split_ratio)\n",
    "X_train, X_test = X_seq[:split], X_seq[split:]\n",
    "y_train, y_test = y_seq[:split], y_seq[split:]\n",
    "\n",
    "# Reshape input dimensions for LSTM\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], X_test.shape[2]))\n",
    "\n",
    "# Function to define the LSTM model\n",
    "def rnn_model(X_train):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(100, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(100, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "    return model, X_train, X_test, y_train, y_test\n",
    "\n",
    "load = 0\n",
    "\n",
    "# Create and train the LSTM model\n",
    "model, X_train, X_test, y_train, y_test = rnn_model(X_train)\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Evaluate the model using test data\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loglikelihood: -189.06\n",
      "AIC: 569380.12\n",
      "BIC: 1416263.85\n",
      "MSE: 0.79\n",
      "RMSE: 0.891\n",
      "Mean Absolute Error (MAE): 0.72\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Function to calculate log-likelihood\n",
    "def calculate_log_likelihood(mse, n):\n",
    "    return -n/2 * np.log(2 * np.pi * mse) - n/2\n",
    "\n",
    "# Inverse transform predictions and test data to original scale\n",
    "y_pred_2 = scaler.inverse_transform(y_pred)\n",
    "y_test_2 = scaler.inverse_transform([y_test])\n",
    "\n",
    "# Function to count trainable parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum([np.prod(K.get_value(w).shape) for w in model.trainable_weights])\n",
    "\n",
    "# Number of parameters in the model\n",
    "num_params = count_parameters(model)\n",
    "\n",
    "# Number of observations\n",
    "n = len(y_test_2[0])\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test_2[0], y_pred_2[:, 0])\n",
    "\n",
    "# Calculate log-likelihood\n",
    "log_likelihood = calculate_log_likelihood(mse, n)\n",
    "\n",
    "# Calculate Akaike Information Criterion (AIC)\n",
    "aic = 2 * num_params - 2 * log_likelihood\n",
    "\n",
    "# Calculate Bayesian Information Criterion (BIC)\n",
    "bic = np.log(n) * num_params - 2 * log_likelihood\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f'Loglikelihood: {round(log_likelihood, 2)}')\n",
    "print(f\"AIC: {round(aic, 2)}\")\n",
    "print(f\"BIC: {round(bic, 2)}\")\n",
    "print(f\"MSE: {round(mse, 2)}\")\n",
    "print(f'RMSE: {round(rmse, 3)}')\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test_2[0], y_pred_2[:, 0])\n",
    "print(f'Mean Absolute Error (MAE): {round(mae, 2)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "      <th>AIC</th>\n",
       "      <th>BIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SARIMAX (3, 1, 2)</td>\n",
       "      <td>2.93</td>\n",
       "      <td>14.60</td>\n",
       "      <td>3.82</td>\n",
       "      <td>4053</td>\n",
       "      <td>4117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LSTM Model</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.89</td>\n",
       "      <td>569380</td>\n",
       "      <td>1416263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model Name   mae    mse  rmse     AIC      BIC\n",
       "0  SARIMAX (3, 1, 2)  2.93  14.60  3.82    4053     4117\n",
       "1         LSTM Model  0.72   0.79  0.89  569380  1416263"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the last row of df_metrics and assign it to df_final\n",
    "df_final = df_metrics[-1:]\n",
    "\n",
    "# Append a new row for the LSTM model to df_final\n",
    "df_final.loc[-1] = ['LSTM Model', round(mae, 2), round(mse, 2), round(rmse, 3), round(aic, 2), round(bic, 2), '']\n",
    "\n",
    "# Drop the 'p-value' column from df_final (if it exists)\n",
    "df_final.drop([\"p-value\"], axis=1, inplace=True)\n",
    "\n",
    "# Reset the index of df_final\n",
    "df_final = df_final.reset_index(drop=True)\n",
    "\n",
    "# Convert 'AIC' column to integer type\n",
    "df_final[\"AIC\"] = df_final[\"AIC\"].astype(int)\n",
    "\n",
    "# Round 'rmse' column to 2 decimal places\n",
    "df_final[\"rmse\"] = df_final[\"rmse\"].round(2)\n",
    "\n",
    "# Convert 'BIC' column to integer type\n",
    "df_final[\"BIC\"] = df_final[\"BIC\"].astype(int)\n",
    "\n",
    "# Display df_final\n",
    "df_final\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
